import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.utils import resample

# Load the dataset
df = pd.read_excel("/content/sample_data/Online Retail.xlsx")

# Drop rows with missing values
df.dropna(inplace=True)

print("K-Means Clustering")
# Select relevant numerical features for clustering (you can customize this)
numerical_cols = ['Quantity', 'UnitPrice']
df = df[numerical_cols]

# Remove negative or zero values
df = df[(df > 0).all(axis=1)]

# Preprocessing functions
def preprocess_raw(data):
    return data.copy()

def preprocess_standard(data):
    scaler = StandardScaler()
    return scaler.fit_transform(data)

def preprocess_minmax(data):
    scaler = MinMaxScaler()
    return scaler.fit_transform(data)

def preprocess_pca(data):
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    pca = PCA(n_components=2)
    return pca.fit_transform(data_scaled)

preprocessing_methods = {
    'Raw': preprocess_raw,
    'StandardScaler': preprocess_standard,
    'MinMaxScaler': preprocess_minmax,
    'PCA after Scaling': preprocess_pca
}

# Clustering and evaluation
results = []

for method_name, preprocess_func in preprocessing_methods.items():
    processed_data = preprocess_func(df)
    
    for k in [3, 4, 5]:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
        labels = kmeans.fit_predict(processed_data)

        #silhouette = silhouette_score(processed_data, labels)
        db_index = davies_bouldin_score(processed_data, labels)
        ch_index = calinski_harabasz_score(processed_data, labels)

        results.append({
            'Preprocessing': method_name,
            'K': k,
            #'Silhouette Score': round(silhouette, 4),
            'Davies-Bouldin Index': round(db_index, 4),
            'Calinski-Harabasz Index': round(ch_index, 4)
        })

# Create and display results table
results_df = pd.DataFrame(results)
print(results_df)


#Hierarchical Clustering
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.cluster import AgglomerativeClustering
from sklearn.exceptions import NotFittedError

# Replace this with actual data loading
# Example: df = pd.read_excel("Online Retail.xlsx")
# For now let's assume df is loaded and contains only numeric values you selected
df = df.dropna()
df = df[['Quantity', 'UnitPrice']]
df = df[(df > 0).all(axis=1)]
df = df.sample(n=10000, random_state=42)  # reduce computation

# Define preprocessing combinations
def preprocess_data(method):
    data = df.copy()
    
    try:
        if method == 'No Processing':
            return data.values
        
        elif method == 'Normalization':
            return MinMaxScaler().fit_transform(data)
        
        elif method == 'Transform':
            return StandardScaler().fit_transform(data)
        
        elif method == 'PCA':
            data = StandardScaler().fit_transform(data)
            return PCA(n_components=2).fit_transform(data)
        
        elif method == 'T+N':
            data = StandardScaler().fit_transform(data)
            return MinMaxScaler().fit_transform(data)
        
        elif method == 'T+N+PCA':
            data = StandardScaler().fit_transform(data)
            data = MinMaxScaler().fit_transform(data)
            return PCA(n_components=2).fit_transform(data)
        
    except Exception as e:
        print(f"Error preprocessing with {method}: {e}")
        return None

methods = ['No Processing', 'Normalization', 'Transform', 'PCA', 'T+N', 'T+N+PCA']
cluster_counts = [3, 4, 5]

# Initialize results
silhouette_list = []
ch_index_list = []
db_index_list = []

for method in methods:
    sil_row, ch_row, db_row = [], [], []

    processed_data = preprocess_data(method)
    
    for k in cluster_counts:
        if processed_data is None:
            sil_row.append('NA')
            ch_row.append('NA')
            db_row.append('NA')
            continue
        
        try:
            cluster = AgglomerativeClustering(n_clusters=k)
            labels = cluster.fit_predict(processed_data)
            
            sil = silhouette_score(processed_data, labels)
            ch = calinski_harabasz_score(processed_data, labels)
            db = davies_bouldin_score(processed_data, labels)

            sil_row.append(round(sil, 2))
            ch_row.append(int(ch))
            db_row.append(round(db, 2))
        except:
            sil_row.append('NA')
            ch_row.append('NA')
            db_row.append('NA')
    
    silhouette_list.extend(sil_row)
    ch_index_list.extend(ch_row)
    db_index_list.extend(db_row)

# MultiIndex for hierarchical header
columns = pd.MultiIndex.from_product(
    [['Using Hierarchical Clustering'], methods, ['c=3', 'c=4', 'c=5']],
    names=["", "", ""]
)

# Combine rows into final table
final_table = pd.DataFrame(
    [silhouette_list, ch_index_list, db_index_list],
    index=['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin'],
    columns=columns
)

# Show table
pd.set_option('display.max_columns', None)
print(final_table)

# Optionally export
# final_table.to_excel("hierarchical_clustering_evaluation.xlsx")


import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Load dataset
df = pd.read_excel("/content/sample_data/Online Retail.xlsx")

# Drop rows with missing CustomerID
df = df.dropna(subset=['CustomerID'])

# Remove returns (invoices starting with 'C')
df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]

# Filter out non-positive values
df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]

# Use only Quantity and UnitPrice for clustering
data = df[['Quantity', 'UnitPrice']]

# Preprocessing methods
preprocessors = {
    'Raw Data': None,
    'StandardScaler': StandardScaler(),
    'MinMaxScaler': MinMaxScaler(),
    'RobustScaler': RobustScaler()
}

k_range = range(2, 6)
results = []

for name, scaler in preprocessors.items():
    if scaler:
        processed_data = scaler.fit_transform(data)
    else:
        processed_data = data.values
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)
        labels = kmeans.fit_predict(processed_data)

        #silhouette = silhouette_score(processed_data, labels)
        db_index = davies_bouldin_score(processed_data, labels)
        ch_index = calinski_harabasz_score(processed_data, labels)

        results.append({
            'Preprocessing': name,
            'K': k,
            #'Silhouette Score': round(silhouette, 4),
            'Davies-Bouldin': round(db_index, 4),
            'Calinski-Harabasz': round(ch_index, 4)
        })

# Create and display results DataFrame
results_df = pd.DataFrame(results)
print(results_df)
